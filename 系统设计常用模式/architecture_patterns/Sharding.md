# Sharding (分片模式)

当单台数据库服务器的磁盘容量、IO 性能或 CPU 算力达到物理极限时，通过将数据集拆分到多个物理独立的数据库节点中。它是大型互联网架构实现 **“无限扩展控制”** 的核武器。

## 1. 痛点：单机数据库的“垂直极限”

虽然现在的服务器很强。但总会有以下时刻：

1.  **容量瓶颈**：你的 `Orders` 表超过了 2TB。即使磁盘装得下，单机的 B+Tree 索引层级已经深不可测，查询慢如牛。
2.  **连接数瓶颈**：1000 个微服务实例同时请求这一个 DB。数据库的连接池被瞬间爆掉。
3.  **IOPS 瓶颈**：即使是用顶级的 NVMe SSD。每秒 10 万次的并发写入也足以让磁盘排队等待到超时。

**本质原因**：单台服务器的资源是线性增长的，但互联网服务的突发流量和数据增长是指数级的。

---

## 2. 解决方案：分而治之，横向移动

我们将完整的表切小，散布到不同的 DB Node 上。

### 规则 A: 垂直拆分 (Vertical Sharding) —— 按模块
*   **做法**：把 `User` 相关表放 A 库，把 `Order` 相关表放 B 库。
*   **解决**：业务间的资源竞争。

### 规则 B: 水平拆分 (Horizontal Sharding) —— 按行
*   **做法**：数据结构不变。按照特定的 **Sharding Key (如 user_id)** 取模。
*   **解决**：单表容量和单机处理能力上限。

---

## 3. 核心挑战：选择 Sharding Key (基因决定论)

**原则**：你以后 90% 的高频查询，都必须带上这个 `Sharding Key`。

*   **正面教材**：用 `user_id` 分片。你查“我的订单”时，系统直接定位到特定的数据库节点。**效率极高。**
*   **负面教材**：用 `order_id` 分片。结果有个查询是“根据用户名查所有订单”。由于系统不知道这个用户在哪个片。必须扫描所有的数据库节点。**这种称为“广播查询”，一旦节点多了，性能瞬间崩盘。**

---

## 4. 无法逃避的副作用 (The Cost)

1.  **无法跨片 Join**：在不同库里的表没法直接写 JOIN。必须通过应用层发起两次查询并在内存里组装。
2.  **分布式事务**：跨分片的事务成了噩梦。必须引入 Saga 或 TCC 等复杂的模式。
3.  **运维复杂度**：扩缩容（Re-sharding）及其痛苦。

---

## 5. 实现方案

*   **客户端分片**：ShardingSphere-JDBC。代码逻辑在应用层处理。
*   **代理层分片**：MyCat / Vitess。应用像连普通 DB 一样。代理层负责路由。
*   **云原生数据库 (Distributed DB)**：TiDB, Spanner。底层原生产生数据均衡，对应用透明。

## 6. 总结
分库分表是大型架构的 **“原子弹”**。
*   **由于代价高昂**：在单机能抗住（比如几百万行数据、主从分离能搞定）时，**坚决不要分片**。
*   **心法**：一旦分出，覆水难收。必须在项目初期就极其慎重地选择分片键。
