z = dot(w,x) + b   特征向量 + 特征权重 + 阈值 

逻辑回归 + 激活函数（提高神经网络的智商） ==》

signmoid

神经网络的得直到自己预测的结果准不准，从而来自动调整自身的学习。（努力学习，依然考不好？？）

损失函数：判断预测结果是否是准确的，针对单个样本的预测精准度的判断
			衡量两个分布之间的距离，一个分布式原始分布或者叫正确的分布（ground truth），
			另一个是目前的分布或模型拟合的分布（prediction）
损失函数：反映模型与实际数据的差距的工具。
    输入x与实际结果是固定的，其实损失函数就是一个关于w与b的一个函数，学习过程就是找到一组（不是一个）w和b使得该损失函数J最小(J是一个凸函数——向下突出).
    梯度下降算法会一步一步的更改w 和 b,最终使得该函数值最小。		

成本函数：整个训练集的预测精度：单个样本的损失累加求平均

如何将数据输入到神经网路：矩阵或者说向量（特征）
神经网路是如何对数据进行预测的：矩阵的运算
预测准确的判断：损失函数、成本函数 

降低损失函数的过程即神经网络的自学习（训练）

dot(w,x) + b： w 与 b决定预测准确性，探寻合适的 w 与 b 即不断学习过程。

如何寻找W,b: 梯度下降(gradient descent)算法，会一步步的更新w和b，最终使得损失函数变得更小。

损失函数是漏斗形（凸函数，向下凸起），训练目的就是遭到漏斗底部的一组w和b。
w' = w - r*dw  (r-学习率或学习步进)
偏导数---斜率---变化比例

计算图：模型的定义及其求解方式，对二者抽象后可以确定一个唯一的计算逻辑，将该逻辑用图表示
		称之为计算图，计算图表现为有向无环图，定义了数据的流转方式，数据的计算方式及各种
		计算之间的相互依赖关系。

神经网路计算= 向前传播 + 反向传播 
向前传播：计算出预测结果及损失
反向传播：计算出损失函数关于每一个参数(w,b)的偏导数，并对这些参数进行梯度下降，使用新参数进行向前传播。

二分类逻辑回归

向量化：大大提升计算速度，减少训练时间，去除代码中的循环
-----------------------------------------------------
标签：要预测的事务，是简单线性回归中的Y变量，如小麦的价格等
特征：输入变量，即简单线性回归中的x变量，简单的可能是1个特征，复杂的可能是数百万不等
样本：数据的特定实例（即x取某个特定的输入），分有标签样本（x,y）、无标签样本(x,?),使用有标签样本训练模型后来预测无标签样本。
模型：定义了特征与标签之间的关系
模型生命周期：
   训练：即向模型输入（展示）有标签样本，让模型学习特征与标签之间的关系。
   推断：即预测，将训练过的模型用于无标签数据的预测（y'）。
特征工程：将原始数据转换为特征矢量（向量的别名）（特征将来要与模型权重相乘，所以某些特征需要转换为数字类型，如名称之类的字符串数据）

有（高）价值的特征：至少在数据集中出现5次
数据清理：剔除坏样本，常见清理手段：缩放特征值、清查（遗漏值，重复样本、不良标签、不良特征值）

逻辑回归 与  线性回归：
   逻辑回归返回的是概率
   分类阈值：即一个分界点（有时候很难判断到底是不是某个分类，如0.6这样的值），过了那个值（如0.6）就是垃圾邮件，那0.6以下都是非垃圾邮件
 

