z = dot(w,x) + b   特征向量 + 特征权重 + 阈值 

逻辑回归 + 激活函数（提高神经网络的智商） ==》

signmoid

神经网络的得直到自己预测的结果准不准，从而来自动调整自身的学习。（努力学习，依然考不好？？）

损失函数：判断预测结果是否是准确的，针对单个样本的预测精准度的判断
			衡量两个分布之间的距离，一个分布式原始分布或者叫正确的分布（ground truth），
			另一个是目前的分布或模型拟合的分布（prediction）
损失函数：反映模型与实际数据的差距的工具。
			

成本函数：整个训练集的预测精度：单个样本的损失累加求平均

如何将数据输入到神经网路：矩阵或者说向量（特征）
神经网路是如何对数据进行预测的：矩阵的运算
预测准确的判断：损失函数、成本函数 

降低损失函数的过程即神经网络的自学习（训练）

dot(w,x) + b： w 与 b决定预测准确性，探寻合适的 w 与 b 即不断学习过程。

如何寻找W,b: 梯度下降(gradient descent)算法，会一步步的更新w和b，最终使得损失函数变得更小。

损失函数是漏斗形（凸函数，向下凸起），训练目的就是遭到漏斗底部的一组w和b。
w' = w - r*dw  (r-学习率或学习步进)
偏导数---斜率---变化比例

计算图：模型的定义及其求解方式，对二者抽象后可以确定一个唯一的计算逻辑，将该逻辑用图表示
		称之为计算图，计算图表现为有向无环图，定义了数据的流转方式，数据的计算方式及各种
		计算之间的相互依赖关系。

神经网路计算= 向前传播 + 反向传播 
向前传播：计算出预测结果及损失
反向传播：计算出损失函数关于每一个参数(w,b)的偏导数，并对这些参数进行梯度下降，使用新参数进行向前传播。

二分类逻辑回归

向量化：大大提升计算速度，减少训练时间，去除代码中的循环
